---
title: "Project 2 Writeup"
date: "2024-12-12"
author: "Team 7"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r}
housing_data <- read.csv("housingdata.csv")
```

**Summarize the most important parts of EDA here**



**Modeling work**


```{r}
library(ggplot2)

#revisit the data 
str(housing_data)

ggplot(housing_data, aes(x = ZHVI)) +
  geom_histogram(bins = 30, fill = "orange", alpha= 0.7) +
  labs(title = "Histogram of ZHVI", x = "ZHVI", y = "frequency")
```

The target variable in our models, ZHVI, doesn't look normally distributed so before moving forward with modeling it's best to transform it to have it be as normally distributed as possible. We will do that below.


```{r}
#Create a new column for log transformed ZHVI
housing_data$log_of_ZHVI <- log(housing_data$ZHVI)


#Result is something that looks more normally distributed
ggplot(housing_data, aes(x = log_of_ZHVI))+
  geom_histogram(bins = 30, fill = 'green', alpha = 0.7) + 
  labs(title = 'Histogram of transformed ZHVI', x = 'log of ZHVI', y = 'frequency')


```

After transforming the target variable, we can observe that the variable isn't perfectly distributed, but it's taken a large step in coming as close as possible to being normally distributed as possible. Therefore, we will move forward with the transformation being our target variable in future models.


**Preparing predictors to work in models**
```{r}
#Creating a year variable and making it numeric
housing_data$year <- substr(housing_data$month, 1, 4)
housing_data$year <- as.numeric(housing_data$year)

#Changing the categorical variables in our data to factors
housing_data$RegionName <- as.factor(housing_data$RegionName)
housing_data$State <- as.factor(housing_data$State)
housing_data$Metro <- as.factor(housing_data$Metro)
housing_data$City <- as.factor(housing_data$City)

#Renaming RegionName to Zipcode since that's what the values of the variable are
colnames(housing_data)[colnames(housing_data) == "RegionName"] <-"Zipcode"

#Check the results and levels to factoring and renaming
summary(housing_data)
str(housing_data)
```
Many of our variables are categorical (not ordinal) variables. We will further discuss the impact this will have later, but for now let's convert these variables to factors of varying levels. The dataset, from Zillow, is mostly comprised of geographical variables of different levels(State, metro area, city, zip code). By levels I mean each variable is a more specific representation than the last. Like how city and states could possibly represent a similar area or have overlap, but city is a more focused area geographically. 

To summarize some key findings we have 43,308 observations with 11 variables now that we've added log of ZHVI and year as variables. Many of the variables we converted into factors have high levels. Namely Zipcode and City. We will later discuss why that isn't good for some of our models later.  



```{r}

#Very rough draft of the model (11/18)
ZHVI_model <- lm(log_of_ZHVI ~ year + State + Metro + Zipcode + SizeRank, data = housing_data)
summary(ZHVI_model)
```

```{r}
#Another model broken into another code chunk because of extensive computation by both models (replacing Zipcode with City)
ZHVI_model_with_city <- lm(log_of_ZHVI ~ year + State + Metro + City + SizeRank, data = housing_data)
summary(ZHVI_model_with_city)

```

With the initial model we've built included the changed variables like state, metro, city, and Zipcode into factor variables. Our first model is testing what we might find from most of the variables being included in the linear model. Kind of like backward stepwise feature selection except we aren't including variables like city which we tested on our own time. Based on what we know about these variables, it would be counterintuitive to include all variables since again many of them are geographic. The result could give us multicollinearity and will definetely result in overfitting since the model will be introduced to noise. Not only that, but the computation for the model is quite extensive at least using a laptop it is.

The result for the model with Zipcode is a model that's very over fitting. With a multiple and adjusted R-squared of 0.996 as well as a residual standard error of 0.0393. 

For the second model where we replaced Zipcode with the variable City instead. The reasoning was simply because City had fewer levels than Zipcode. Here we observed a multiple R-squared of 0.9299 and an adjusted R-squared of 0.9283. For this model, it's worth double checking and testing overfitting. We excluded it from our presentation for the sake of the audience's ability to interpret it and model complexity. We will test overfitting below. 


```{r}
# Perform k-fold cross-validation
library(caret)

#Remove NAs from ZHVI and log_of_ZHVI
housing_data_clean <- na.omit(housing_data)


set.seed(123)
cv_results <- train(
  log_of_ZHVI ~ year + State + Metro + City + SizeRank,
  data = housing_data_clean,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)

# Print cross-validation results
print(cv_results)

# Extract the cross-validated R-squared
cat("Cross-validated R-squared: ", max(cv_results$results$Rsquared), "\n")
```

The result from running this code is a ton of warnings saying "Warning in predict.lm(modelFit, newdata) :
  prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases". The result means we're encountering collinearity issues or if the model has redundant variables which isn't surprising. We did previously discuss how we might get collinearity since variables like city or zipcode will have some overlap in representing some of same area. 
  
To add to all this, the model is just going to be more complex than we'd really like. So our next step is to adjust and simplify the model and ditch the following model.


**Simple Model 1 - Linear Regression without City or Zipcode**

```{r}
#Avoiding factor variables with a large number of levels
ZHVI_model_simple <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank, data = housing_data)
summary(ZHVI_model_simple)
```
After simplifying the model by removing RegionName(zipcode) we see that the r-squared has dropped significantly to 0.576 and the adjusted r-squared is now 0.576. Before checking to make the model even simpler, we want to check the residuals.


**Check model with the same variables, this time adding interactions**
```{r}
ZHVI_model_interaction <- lm(log_of_ZHVI ~ year * State + year * Metro + SizeRank * Metro, data = housing_data)
summary(ZHVI_model_interaction)
```

From checking the interaction between predictors like year and state or year and metro among other interactions. The new model explains slightly more variance in the data (60.2%). We still believe there's room for improvement though. 

Conclusions from interactions: 

Year does not have a significant impact on ZHVI on it's own. Of course this is to be expected since year doesn't have the same uniform affect across all areas. (Coefficient=2.33e-03, p= 0.89596)

As discussed before, compared to DC states MD and VA have negative effects on ZHVI. Of course metro terms are significant signaling year has a different impact on different metro areas. 

SizeRank and Metro interactions were highly significant. Metro areas with larger cities (lower SizeRank) have a stronger positive impact on ZHVI while others might be smaller or negative. Metro areas like Baltimore-Columbia-Towson, had positive coefficients while more rural areas like Big Stone Gap, VA(population 5,114) have negative impacts on ZHVI.


With that being said, we want to move forward by keeping the model as simple as possible. We decide that the trade-off between having interaction terms and the increase of our multiple and adjusted r-squared values isn't worth it. We move foward with the linear regression model from before that didn't have interaction terms.

**Checking assumptions of linear regression models from our initial model with 4 predictors no interactions**
```{r}
#Checking residuals, normality, homoscedacity

# Residuals vs. Fitted Values Plot
plot(ZHVI_model_simple$fitted.values, resid(ZHVI_model_simple),
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

# Q-Q Plot for Normality of Residuals
qqnorm(resid(ZHVI_model_simple), main = "Q-Q Plot of Residuals")
qqline(resid(ZHVI_model_simple), col = "red")

# Scale-Location Plot
plot(ZHVI_model_simple$fitted.values, sqrt(abs(resid(ZHVI_model_simple))),
     xlab = "Fitted Values", ylab = "Square Root of |Residuals|",
     main = "Scale-Location Plot")
abline(h = 0, col = "red")

#Check normality
ks.test(resid(ZHVI_model_simple), "pnorm", 
        mean = mean(resid(ZHVI_model_simple)), 
        sd = sd(resid(ZHVI_model_simple)))
#Check Homoscedasticity
install.packages("lmtest") # If not already installed
library(lmtest)
bptest(ZHVI_model_simple)

```
Following the residuals vs fitted plot and the q-q plot, we observe there is some heteroscedasticity. The q-q plot also shows there are a few outliers in the dataset. 

This is to be expected since ZHVI was not normally distributed. Taking the log transformation of ZHVI helped, but didn't make it perfectly normally distributed. Let's check the mutlicollinearity. 

**Residuals vs Fitted Plot for model with interaction terms**
```{r}
# Step 1: Extract residuals and fitted values
residuals_interaction <- residuals(ZHVI_model_interaction)
fitted_values_interaction <- fitted(ZHVI_model_interaction)

# Step 2: Create Residuals vs Fitted plot
plot(
  fitted_values_interaction,
  residuals_interaction,
  xlab = "Fitted Values",
  ylab = "Residuals",
  main = "Residuals vs Fitted (Interaction Model)",
  pch = 20,
  col = "blue"
)
abline(h = 0, col = "red", lwd = 2) # Add horizontal reference line

```

```{r}
#Checking for Multicollinearity
install.packages("car")
library(car)

vif(ZHVI_model_simple)

```

Based on the output of the vif stats, we observe that the VIF for all variables is below 5 and is acceptable. We can move forward with adjusting the model based on this. Let us try and test the model by splitting the data by our newly created variable, year. We will train our data on the years 2021 and 2022 and test it on the year 2023. 

```{r}
#Split into training and testing by year
training_data <- subset(housing_data, year %in% c(2021, 2022))
testing_data <- subset(housing_data, year == 2023)

ZHVI_model_train <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank, data = training_data)
predictions <- predict(ZHVI_model_train, newdata = testing_data)



valid_data <- na.omit(data.frame(actuals, predictions))


valid_actuals <- valid_data$actuals
valid_predictions <- valid_data$predictions

# Calculate residuals
valid_residuals <- valid_actuals - valid_predictions

# Compute performance metrics
MAE <- mean(abs(valid_residuals))   # Mean Absolute Error
RMSE <- sqrt(mean(valid_residuals^2)) # Root Mean Squared Error
R2 <- cor(valid_predictions, valid_actuals)^2  # R-squared

cat("MAE:", MAE, "\n")
cat("RMSE:", RMSE, "\n")
cat("R-squared:", R2, "\n")

#Scatter plot
plot(
  test_data_2023$log_of_ZHVI, test_data_2023$predicted,
  xlab = "Actual Log(ZHVI)", ylab = "Predicted Log(ZHVI)",
  main = "Actual vs Predicted Log(ZHVI) for 2023",
  pch = 16, col = "blue"
)
abline(0, 1, col = "red", lwd = 2) # Add a 45-degree line for perfect prediction

# Line plot using ggplot2
library(ggplot2)
ggplot(test_data_2023, aes(x = 1:nrow(test_data_2023))) +
  geom_line(aes(y = log_of_ZHVI, color = "Actual")) +
  geom_line(aes(y = predicted, color = "Predicted")) +
  labs(
    x = "Index",
    y = "Log(ZHVI)",
    title = "Trends in Actual vs Predicted Log(ZHVI) for 2023"
  ) +
  scale_color_manual(values = c("Actual" = "blue", "Predicted" = "red")) +
  theme_minimal()
```
To fully test how our model performs on unseen data, we decided to split the data into training and testing sets based on years as mentioned before. As shown above, we gave the years 2021-2022 into the training set and 2023 into the test set to observe how well our model will work on unseen data. The result is the model explains 56% of the variance in log(ZHVI) with a MAE of 0.293 and RMSE of 0.39. 

The results suggest that housing data in the real world can be very complex. Things like income, housing supply, houshold size, crime, and more were not included in the data Zillow provides from their website directly. This could explain the plateu we're witnessing when adding more variables from what we presently have.


**Testing the same on the model with interaction terms**
```{r}
# Split into training and testing by year
training_data <- subset(housing_data, year %in% c(2021, 2022))
testing_data <- subset(housing_data, year == 2023)

# Fit the model with interaction terms
ZHVI_model_interaction_train <- lm(log_of_ZHVI ~ year * State * Metro * SizeRank, data = training_data)

# Make predictions on the testing data
predictions_interaction <- predict(ZHVI_model_interaction_train, newdata = testing_data)

# Actual values for testing
actuals_interaction <- testing_data$log_of_ZHVI

# Calculate residuals
test_residuals_interaction <- actuals_interaction - predictions_interaction

# Evaluate performance using MAE, RMSE, and R-squared
MAE_interaction <- mean(abs(test_residuals_interaction))   # Mean Absolute Error
RMSE_interaction <- sqrt(mean(test_residuals_interaction^2)) # Root Mean Squared Error
R2_interaction <- cor(predictions_interaction, actuals_interaction)^2  # R-squared on test data

# Print results
cat("MAE (Interaction Model):", MAE_interaction, "\n")
cat("RMSE (Interaction Model):", RMSE_interaction, "\n")
cat("R-squared (Interaction Model):", R2_interaction, "\n")

# Prepare testing data with predictions for further analysis
testing_data$predicted <- predictions_interaction
```


To move forward with modeling this dataset, we need to determine the following: 

1. Is linear regression the problem for this dataset? (Is the model type too simple for our dataset)

2. Can we witness a higher r-squared value by using another model type?

3. How can we build a model that has stronger predictive power for our target variable? 

Let us test if the linear model itself is the problem by using random forest. Random forest is a bit more suited for complex models than the linear model, so naturally we could expect a better explanation of the variance in our target variable. 

```{r}
# Load necessary library for random forest
library(randomForest)

# Remove rows with missing values
training_data_clean <- na.omit(training_data)
testing_data_clean <- na.omit(testing_data)

# Check for missing values after removing them
sum(is.na(training_data_clean))
sum(is.na(testing_data_clean))

# Random forest with the same variables
rf_model_continuous <- randomForest(
  log_of_ZHVI ~ year + State + Metro + SizeRank,
  data = training_data_clean,
  importance = TRUE,  # To calculate variable importance
  ntree = 100         # Number of trees
)



# Predictions
rf_predictions_continuous <- predict(rf_model_continuous, newdata = testing_data_clean)

# Performance Metrics
rf_residuals_continuous <- testing_data_clean$log_of_ZHVI - rf_predictions_continuous
MAE_rf_continuous <- mean(abs(rf_residuals_continuous))
RMSE_rf_continuous <- sqrt(mean(rf_residuals_continuous^2))
R2_rf_continuous <- cor(rf_predictions_continuous, testing_data_clean$log_of_ZHVI)^2

# Print metrics
cat("Random Forest with Continuous SizeRank:\n")
cat("MAE:", MAE_rf_continuous, "\nRMSE:", RMSE_rf_continuous, "\nR-squared:", R2_rf_continuous, "\n")

# Variable Importance
importance(rf_model_continuous)
varImpPlot(rf_model_continuous)

```
We observe from the random forest model explains slightly more of the variance, but doesn't really have better metrics in MAE or RMSE. It's possible the data relationships aren't totally linear. We do observe that Metro and State are very important features in explaining log(ZHVI).

What we can conclude from this is that again, housing data in the real world is very complex.

```{r}
library(caret)

training_data_clean <- na.omit(training_data)

# Set up cross-validation
tuneGrid <- expand.grid(mtry = c(1, 2, 3, 4))

# Train the model using cross-validation
rf_model_tuned <- train(
  log_of_ZHVI ~ year + State + Metro + SizeRank,
  data = training_data_clean,
  method = "rf",        # Random Forest method
  trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation
  tuneGrid = tuneGrid,   # Use the tuning grid for mtry
  importance = TRUE
)

# Check the best model and performance
print(rf_model_tuned)
```




```{r}

library(caret)


# Set up cross-validation with 10 folds
train_control <- trainControl(method = "cv", number = 10)

# Fit the model with City included
model_with_city <- train(log_of_ZHVI ~ year + State + Metro + City, data = housing_data,
                         method = "lm", trControl = train_control)

# View the results of cross-validation
print(model_with_city)


```
Result: Error in na.fail.default(list(log_of_ZHVI = c(12.8550329145667, 12.6063557512654,  : 
  missing values in object


```{r}
library(leaps)

# Forward stepwise selection
forward_selection <- regsubsets(
  log_of_ZHVI ~ year + State + Metro + City + SizeRank, # Full set of predictors
  data = housing_data,        # Your dataset
  method = "forward",         # Specify forward stepwise selection
  nvmax = 10                  # Maximum number of predictors to include
)

# Summary of the forward selection process
summary_forward <- summary(forward_selection)

# Display the results
print(summary_forward)

# Get the best model for each number of predictors
best_models <- summary_forward$which
print(best_models)

# Plotting the adjusted R-squared to visualize the best number of predictors
plot(summary_forward$adjr2, type = "b", xlab = "Number of Predictors", ylab = "Adjusted R-squared")

```

```{r}
forward_model_with_city <- regsubsets(
    log_of_ZHVI ~ year + State + Metro + SizeRank + City,
    data = housing_data,
    nvmax = 12,  # Adjust if you want to limit the number of predictors
    method = "forward"
)
summary_forward_city <- summary(forward_model_with_city)

coef(forward_model_with_city, which.max(summary_forward_city$adjr2))
```

After testing  this, we can observe that overfitting is a problem with models of this dataset. Many of the variables given to us in this dataset are geographical. So for example, state, city, metro, and zipcode are all geographical measures of different degrees. Including all, or certain variables with a high number of levels introduces our model to noise and leads to overcomplexity and overfitting. 


```{r}
library(caret)
library(ggplot2)
library(lattice)

housing_data_clean <- na.omit(housing_data)


control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation



model_with_city <- train(log_of_ZHVI ~ year + State + Metro + City, 
                         data = housing_data_clean, 
                         method = "lm", 
                         trControl = control)

model_without_city <- train(log_of_ZHVI ~ year + State + Metro, 
                             data = housing_data_clean, 
                             method = "lm", 
                             trControl = control)


print(model_with_city)
print(model_without_city)
```
Code took forever to run further proving that many of the variables with an incredibly high level of factors have led to longer computation times, overfitting, and more model complexity.




```{r}
# Model with City
summary(lm(log_of_ZHVI ~ year + State + Metro + City, data = housing_data))

# Model without City
summary(lm(log_of_ZHVI ~ year + State + Metro, data = housing_data))

```

Result: The model with city had a multiple R-squared of 0.93 and an adjusted R-squared of 0.928. It also returned "5 not defined because of singularities". Given the many levels of some of the variables, it might be better to just simplify the model.

```{r}
# Load the car package
library(car)

# Fit the linear model
housing_model <- lm(log_of_ZHVI ~ year + State + Metro + City, data = housing_data)

# Calculate VIF for each predictor
vif_values <- vif(housing_model)

# View the VIF values
print(vif_values)
```

Testing for multicollinearity in this model with City included we get the error "there are aliased coefficients in the model". An error that occurs when there is perfrect multicollinearity in the model.

```{r}
housing_data <- na.omit(housing_data)


# Split the data into training and testing sets
set.seed(123)  # Set seed for reproducibility
trainIndex <- createDataPartition(housing_data$log_of_ZHVI, p = 0.8, list = FALSE)
train_data <- housing_data[trainIndex, ]
test_data <- housing_data[-trainIndex, ]

# Load necessary libraries
library(caret)
library(glmnet)

# Set up cross-validation control
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Ridge regression model (L2 regularization)
model_ridge <- train(log_of_ZHVI ~ year + State + Metro + City, 
                     data = train_data,  # Use the training set
                     method = "glmnet", 
                     trControl = control, 
                     tuneGrid = expand.grid(alpha = 0, lambda = seq(0.0001, 1, length = 100)))

# Print model results
print(model_ridge)
```


Based on the many different steps we've tried by now, we can see many of our variables we thought would contribute meaningfully to our linear model has led to problems in overfitting, complexity, and computation. So we decided maybe to take a different approach and add some variables Zillow didn't give to us before. 


```{r}
# Load ACS data and your housing dataset
acs_data <- read.csv("Merged_ZHVI_ACS.csv")
housing_data <- read.csv("C:/Users/shime/Downloads/zhvi-all-homes_md-va-dc_2021-2023.csv")

# Convert 'zhvi' in ACS data: Remove '$' and commas, then convert to numeric
acs_data$zhvi <- as.numeric(gsub("[\\$,]", "", acs_data$zhvi))

# Ensure 'RegionName' in housing_data matches 'zipcode' in acs_data
housing_data$RegionName <- as.character(housing_data$RegionName)
acs_data$zipcode <- as.character(acs_data$zipcode)

# Merge datasets using 'RegionName' and 'zipcode' as the key
merged_data <- merge(housing_data, acs_data, by.x = "RegionName", by.y = "zipcode", all.x = TRUE)

# Feature Engineering: Add derived variables
merged_data$income_per_household <- merged_data$averageincome / merged_data$households
#Check if average_income is skewed
hist(merged_data$averageincome, main = "Histogram of Average Income", xlab = "Average Income")
merged_data$log_averageincome <- log(merged_data$averageincome)

# Save the enhanced dataset
write.csv(merged_data, "enhanced_housing_data.csv", row.names = FALSE)

```

We have merged data from ACS(American Community Survey) under the U.S. Census Bureau to bring in two new variables in our dataset. The variables we will be focusing on are the average household income in a given zipcode and the number of homes in a given zip code. Each of the variables aren't monitored monthly like the ZHVI data we had. Another problem is that the variables only go up until 2022. The data for 2023 isn't readily available yet at the time of writing.

The goal in adding these variables is for more of the variance in log(ZHVI) to be explained. From the context of housing as a topic, we know supply is said to have an impact on price as well as average household income. 

Modeling should check if these variabels are statistically significant and if they have strong coefficneints/impacts on the target variable.


```{r}
#Building the new linear model with the two new variables from the merged data
new_housing_model <- lm(log_of_ZHVI ~ averageincome + households + State + Metro + SizeRank, data = merged_data)
summary(new_housing_model)

```

Compared to the original linear model with only geographical predictors, this model is significantly improved. The Multiple R-squared is at 0.763 and the adjusted is the same. 

**CHECK THIS BEFORE SUBMISSION**
Breaking down coefficients: A 1% increase in the average income corresponds to an increase in the log_ZHVI by about 0.86%. To no surprise coefficients for MD and VA are negative meaning they have a lower log_of_ZHVI compared to the reference state of DC.

```{r}
interaction_model <- lm(log_of_ZHVI ~ averageincome * households + averageincome * State + averageincome * Metro + SizeRank, data = merged_data)
summary(interaction_model)
```

The interaction term between log(averageincome) and households is significant, with a p-value of 1.5e-06. This suggests that the relationship between average income and ZHVI may depend on the number of households.

Many interaction terms between log(averageincome) and the State or Metro variables are also significant, indicating that the effect of income on ZHVI differs across various regions and metros.

These results support the idea that the relationship between income and housing values is not uniform across different areas, and adjusting for these interactions could improve model accuracy.

The multiple and adjusted r-squared are now 0.789.


**More testing for new model without interaction terms**
```{r}
# Split into training and testing by year
#training_data <- subset(merged_data, year %in% c(2021, 2022))
#testing_data <- subset(merged_data, year == 2023)

# Check for NAs in the testing data
sum(is.na(testing_data))

# Fit the linear model
new_housing_model <- lm(log_of_ZHVI ~ averageincome + households + State + Metro + SizeRank, data = training_data)

# Predictions on testing data
predictions_new <- predict(new_housing_model, newdata = testing_data)

# Actual values and residuals
actuals_new <- testing_data$log_of_ZHVI
test_residuals_new <- actuals_new - predictions_new

# Error metrics
MAE_new <- mean(abs(test_residuals_new))   # Mean Absolute Error
RMSE_new <- sqrt(mean(test_residuals_new^2)) # Root Mean Squared Error
R2_new <- cor(predictions_new, actuals_new)^2  # R-squared on test data

cat("MAE:", MAE_new, "\n")
cat("RMSE:", RMSE_new, "\n")
cat("R-squared:", R2_new, "\n")

# Scatter plot for Actual vs Predicted Log(ZHVI) for 2023
plot(
  testing_data$log_of_ZHVI, predictions_new,
  xlab = "Actual Log(ZHVI)", ylab = "Predicted Log(ZHVI)",
  main = "Actual vs Predicted Log(ZHVI) for 2023 (New Model)",
  pch = 16, col = "blue"
)
abline(0, 1, col = "red", lwd = 2) # Add a 45-degree line for perfect prediction
```


**Summarized write up and conclusion**

Our SMART question was a bit of both inference and prediction. We acknowledge that home value/price data can be very complex in the real world with many factors holding varying influence over the given value of a home. The work we did modeling in this project is direct proof of this. Not only did we use the model to try and determine what exactly influences the value of Zillow home value, but we tried to use the influences we found to test if we can build a model that can decently predict so. By taking our model and splitting training and testing data by certain years of our dataset we were able to see the predictive power of the model by comparing its results with our actual data for 2023. 
