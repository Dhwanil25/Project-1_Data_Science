---
title: '"Group 7 Project one EDA file"'
author: "Yonathan Shimelis"
date: "2024-10-15"
output: html_document
---

```{r init, include=F}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
# Once installed, load the library.
library(ezids)
```


```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
options(scientific=T, digits = 3) 
```

# Step 1 and 1a

**Loading the csv/data into the RMD file, observing summary statistics and structure.**

```{r Chunk one, Yonathan}
#Chunk one, let's use this for basic EDA, initial understanding of the data
housing_data <- read.csv("C:/Users/shime/Downloads/zhvi-all-homes_md-va-dc_2021-2023.csv")


```
We know that the dataset comes from Zillow. The most important variable, ZHVI stands for zillow home value index. ZHVI represents home values in the 35th to 65th percentile of a given geographic region. When extracting our raw data, we chose the most specific geographic metric, zip code. ZHVI is determined from a variety of metrics like recent home sales, zillow


**Check out some of the information in the dataset**
```{r Chunk 1a Yonathan}
str(housing_data)
summary(housing_data)
```
Our dataset has `r nrow(housing_data)` observations
We've also observed 9 variables, but likely won't be using all of them. 
We have null values for ZHVI. There are a few other interesting things about the dataset. Including, the minimum ZHVI for all three regions throughout 2021-2023 is just 25,919. The maximum is 2,829,491. 

# Step 2 

**Break down the dataset by regions.**

```{r Chunk two}
#Using code we group the ZHVIs for each data together by regions/states, DC, MD, and VA


dc_data <- subset(housing_data, State == "DC")
summary(dc_data)

md_data <- subset(housing_data, State == "MD")
summary(md_data)

va_data <- subset(housing_data, State == "VA")
summary(va_data)

```
Maryland and Virginia are the only of the three that has NA's with a total of 468 NAs'and 1028 NA's. The most expensive home in the dataset is located in Maryland


# Step 3

**Finding the average ZHVI of each region over all three years as a whole**
```{r Chunk three} 
#Chunk three, further breakdown 

#Average ZHVI of dc housing over all 3 years
mean_dc <- mean(dc_data$ZHVI, na.rm = TRUE)
mean_dc

#Average ZHVI of va housing over all 3 years
mean_va <- mean(va_data$ZHVI, na.rm = TRUE)
mean_va

#Average ZHVI of md housing over all 3 years
mean_md <- mean(md_data$ZHVI, na.rm = TRUE)
mean_md


```
We can see the average home value from 2021-2023 was the largest in DC (650863), Maryland(420935) and then Virginia(314345)

# Step 4  

**Separating the DC, Maryland, and Virginia data by year**
```{r Chunk 4}
#Create column 'Year' by taking the first 4 characters of the variable 'month'
dc_data$Year <- substr(dc_data$month, 1, 4)
va_data$Year <- substr(va_data$month, 1, 4)
md_data$Year <- substr(md_data$month, 1, 4)

#Creating subsets of dc data by year
dc_data_2021 <- subset(dc_data, Year == "2021")
dc_data_2022 <- subset(dc_data, Year == "2022")
dc_data_2023 <- subset(dc_data, Year == "2023")

#Finding average ZHVI of dc by year
mean_dc_2021 <- mean(dc_data_2021$ZHVI, na.rm = TRUE)
mean_dc_2022 <- mean(dc_data_2022$ZHVI, na.rm = TRUE)
mean_dc_2023 <- mean(dc_data_2023$ZHVI, na.rm = TRUE)

#Creating subsets of VA data by year
va_data_2021 <- subset(va_data, Year == "2021")
va_data_2022 <- subset(va_data, Year == "2022")
va_data_2023 <- subset(va_data, Year == "2023")


#Finding average ZHVI of VA by year
mean_va_2021 <- mean(va_data_2021$ZHVI, na.rm = TRUE)
mean_va_2022 <- mean(va_data_2022$ZHVI, na.rm = TRUE)
mean_va_2023 <- mean(va_data_2023$ZHVI, na.rm = TRUE)


#Creating subsets of MD data by year
md_data_2021 <- subset(md_data, Year == "2021")
md_data_2022 <- subset(md_data, Year == "2022")
md_data_2023 <- subset(md_data, Year == "2023")


#Finding average ZHVI of MD by year
mean_md_2021 <- mean(md_data_2021$ZHVI, na.rm = TRUE)
mean_md_2022 <- mean(md_data_2022$ZHVI, na.rm = TRUE)
mean_md_2023 <- mean(md_data_2023$ZHVI, na.rm = TRUE)
```


# Step 5

**Finding the average ZHVI of each reagion for each given year**
```{r Chunk 5}
#Comparing values from chunk four
options(scipen = 999)

#Average ZHVI of DC for years 2021-2023
mean_dc_2021
mean_dc_2022
mean_dc_2023

#Average ZHVI of VA for years 2021-2023
mean_va_2021
mean_va_2022
mean_va_2023

#Average ZHVI of MD for years 2021-2023
mean_md_2021
mean_md_2022
mean_md_2023
```

From this we observe the average housing price in DC wasn't always increasing it went from 643,242 in 2021 to 662,298 in 2022 and then back down to 647,048 in 2023

Virginia however, was always increasing. Going from 293,828 to 319,719 in 2022 and 328,925 in 2023

Maryland is the same with housing going from 400,198 to 426,621 and 434,221 in 2023.

# Step 6

**Plotting the data from chunk 4 and 5**
```{r Chunk 6}

#Plotting ZHVI for each region by year
library(ggplot2)

#Creating a data frame so all the data is put into one for it to be able to be plotted
combined_data <- rbind(dc_data, md_data, va_data)

ggplot(combined_data, aes(x = substr(month, 1, 4), y = ZHVI, fill = State)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  labs(
    title = "Average Housing Prices (ZHVI) by State/Region Over the Years",
    x = "Year",
    y = "Average ZHVI",
    fill = "State/Region"
  ) +
  scale_y_continuous(
    breaks = seq(200000, 650000, by = 100000)  #Making the scale smaller than the default
  )
```


# Step 7
**First statistical testing on the ZHVI of the three regions from 2021-2023 as a whole**
```{r Chunk 7}
# statistical testing the average home price for all three states/regions using the combined data


anova_result <- aov(ZHVI ~ State, data = combined_data)
summary(anova_result)

tukey_result <- TukeyHSD(anova_result)
tukey_result
```
With a p-value below 0.05 we can reject the null hypothesis that the means of the ZHVI for the three regions/states are equal. After doing a post-hoc test, we can see the average housing price(ZHVI) in Maryland is 229,928 less than DC. Housing price in VA is 336,518 less than DC. And VA is 106,590 cheaper than MD. There are statistically significant differences in ZHVI between all of the regions.

# Step 8

**Statistical testing on the ZHVI of each region for each given year**
```{r Chunk 8}

#Creating sets of data with all regions for each given year within the range of 2021-2023
all_2021 <- rbind(dc_data_2021, va_data_2021, md_data_2021)

all_2022 <- rbind(dc_data_2022, va_data_2022, md_data_2022)

all_2023 <- rbind(dc_data_2023, va_data_2023, md_data_2023)


#Anova test for the year 2021
anova_2021 <- aov(ZHVI ~ State, data = all_2021)
summary(anova_2021)

#Anova test for the year 2022
anova_2022 <- aov(ZHVI ~ State, data = all_2022)
summary(anova_2022)

#Anova test for the year 2023
anova_2023 <- aov(ZHVI ~ State, data = all_2023)
summary(anova_2023)

#Post HOC test for 2021
tukey_result2021 <- TukeyHSD(anova_2021)
tukey_result2021

#Post HOC test for 2022
tukey_result2022 <- TukeyHSD(anova_2022)
tukey_result2022

#Post HOC test for 2023
tukey_result2023 <- TukeyHSD(anova_2023)
tukey_result2023
```
Every year has a p-value below 0.05. 

We observe a 243,045 difference in DC-MD homes, a 349414 difference in VA-DC homes, and a 106369 difference in VA-MD homes for the year 2021. 

For 2022 we observe an avg difference of 235,637 between MD-DC homes, 342579 between VA-DC homes, and 106942 between VA-MD homes.

For 2023 we observe a 212827 diff between MD-DC homes, 318123 for VA-DC homes, and 105296 for VA-MD homes. 

All years and regions had p-values below 0.05 so there are significant differences between all the avg housing prices between every region regardless of the year.


# Step 9 

**Further statistical testing**

```{r Chunk 9}
#ANOVA testing the average ZHVI of a region in one year versus the same region in the other years of the data
#We decided to take this step on Dr.Morris' recommendation following our first presentation of the data. 

anova_dc_all_years <- aov(ZHVI ~ Year, data = dc_data)
summary(anova_dc_all_years)

anova_md_all_years <- aov(ZHVI ~ Year, data = md_data)
summary(anova_md_all_years)

anova_va_all_years <- aov(ZHVI ~ Year, data = va_data)
summary(anova_va_all_years)

```
The p-value for DC is above 0.05, meaning there is no significant difference in the average ZHVI of DC between the years 2021,2022, and 2023. 

The p-value for md and va were both below 0.05. This means we reject the null hypothesis and that there is a difference in the average ZHVI of of MD for at least one of the years (2021-2023) compared to the others. This result is the same for VA. 

```{r}
#Tukey Tests for the p-values below 0.05

tukey_md <- TukeyHSD(anova_md_all_years)
print(tukey_md)
  
tukey_va <- TukeyHSD(anova_va_all_years)
print(tukey_va)

```
The result for maryland confirms that the average ZHVI in the state in the year 2022 was significantly higher than in 2021. The same goes for the average ZHVI in MD from 2023 being significantly higher than 2021. However, the average ZHVI for maryland in 2023 is not signficantly higher than the average ZHVI in 2022. 

For Virginia the average ZHVI in 2022 was significantly higher than in 2021. The average ZHVI in 2023 is significantly higher than in 2021. The average ZHVI in 2023 is also significantly higher than in 2022.



*Prep for write-up*

**Modeling work**


```{r}
library(ggplot2)

#revisit the data 
str(housing_data)

ggplot(housing_data, aes(x = ZHVI)) +
  geom_histogram(bins = 30, fill = "orange", alpha= 0.7) +
  labs(title = "Histogram of ZHVI", x = "ZHVI", y = "frequency")
```

The target variable in our models, ZHVI, doesn't look normally distributed so I'm creating a column with it transformed to look more normally distributed. 


```{r}
housing_data$log_of_ZHVI <- log(housing_data$ZHVI)


#Result is something that looks more normally distributed
ggplot(housing_data, aes(x = log_of_ZHVI))+
  geom_histogram(bins = 30, fill = 'green', alpha = 0.7) + 
  labs(title = 'Histogram of transformed ZHVI', x = 'log of ZHVI', y = 'frequency')


```

After transforming the target variable, we can observe that the variable isn't perfectly distributed, but it's taken a large step in coming as close as possible to being normally distributed as possible. Therefore, we will move forward with the transformation being our target variable.

**Preparing predictors to work in models**
```{r}
#Creating a year variable and making it numeric
housing_data$year <- substr(housing_data$month, 1, 4)
housing_data$year <- as.numeric(housing_data$year)

#Changing the categorical variables in our data to factors
housing_data$RegionName <- as.factor(housing_data$RegionName)
housing_data$State <- as.factor(housing_data$State)
housing_data$Metro <- as.factor(housing_data$Metro)
housing_data$City <- as.factor(housing_data$City)
```


```{r}

#Very rough draft of the model (11/18)
ZHVI_model <- lm(log_of_ZHVI ~ year + State + Metro + RegionName, data = housing_data)
summary(ZHVI_model)
```

With the initial model we've built we've changed variables like state, metro, city, and RegionName(zipcode) into factor variables. Our first model is testing what we might find from most of the variables being included in the linear model. The result is a model that's very over fitting. With a mutliple and adjusted R-squared of 0.996 as well as a residual standard error of 0.0393. So our next step is to adjust and simplify the model and ditch the following model.


**Model 1 - Predicting ZHVI over all three regions as a whole**

```{r}
#Avoiding factor variables with a large number of levels
ZHVI_model_simple <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank, data = housing_data)
summary(ZHVI_model_simple)
```
After simplifying the model by removing RegionName(zipcode) we see that the r-squared has dropped significantly to 0.576 and the adjusted r-squared is now 0.576. Before checking to make the model even simpler, we want to check the residuals.


**Check model with the same variables, this time adding interactions**
```{r}
ZHVI_model_interaction <- lm(log_of_ZHVI ~ year * State + year * Metro + SizeRank * Metro, data = housing_data)
summary(ZHVI_model_interaction)
```

From checking the interaction between predictors like year and state or year and metro among other interactions. The new model explains slightly more variance in the data (60.2%). We still believe there's room for improvement though. 

Conclusions from interactions: 

Year does not have a significant impact on ZHVI on it's own. Of course this is to be expected since year doesn't have the same uniform affect across all areas. (Coefficient=2.33e-03, p= 0.89596)

As discussed before, compared to DC states MD and VA have negative effects on ZHVI. Of course metro terms are significant signaling year has a different impact on different metro areas. 

SizeRank and Metro interactions were highly significant. Metro areas with larger cities (lower SizeRank) have a stronger positive impact on ZHVI while others might be smaller or negative. Metro areas like Baltimore-Columbia-Towson, had positive coefficients while more rural areas like Big Stone Gap, VA(population 5,114) have negative impacts on ZHVI.


With that being said, we want to move forward by keeping the model as simple as possible. We decide that the trade-off between having interaction terms and the increase of our multiple and adjusted r-squared values isn't worth it. We move foward with the linear regression model from before that didn't have interaction terms.

**Checking assumptions of linear regression models from our initial model with 4 predictors no interactions**
```{r}
#Checking residuals, normality, homoscedacity

# Residuals vs. Fitted Values Plot
plot(ZHVI_model_simple$fitted.values, resid(ZHVI_model_simple),
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

# Q-Q Plot for Normality of Residuals
qqnorm(resid(ZHVI_model_simple), main = "Q-Q Plot of Residuals")
qqline(resid(ZHVI_model_simple), col = "red")

# Scale-Location Plot
plot(ZHVI_model_simple$fitted.values, sqrt(abs(resid(ZHVI_model_simple))),
     xlab = "Fitted Values", ylab = "Square Root of |Residuals|",
     main = "Scale-Location Plot")
abline(h = 0, col = "red")

#Check normality
ks.test(resid(ZHVI_model_simple), "pnorm", 
        mean = mean(resid(ZHVI_model_simple)), 
        sd = sd(resid(ZHVI_model_simple)))
#Check Homoscedasticity
install.packages("lmtest") # If not already installed
library(lmtest)
bptest(ZHVI_model_simple)

```
Following the residuals vs fitted plot and the q-q plot, we observe there is some heteroscedasticity. The q-q plot also shows there are a few outliers in the dataset. 

This is to be expected since ZHVI was not normally distributed. Taking the log transformation of ZHVI helped, but didn't make it perfectly normally distributed. Let's check the mutlicollinearity. 

**Residuals vs Fitted Plot for model with interaction terms**
```{r}
# Step 1: Extract residuals and fitted values
residuals_interaction <- residuals(ZHVI_model_interaction)
fitted_values_interaction <- fitted(ZHVI_model_interaction)

# Step 2: Create Residuals vs Fitted plot
plot(
  fitted_values_interaction,
  residuals_interaction,
  xlab = "Fitted Values",
  ylab = "Residuals",
  main = "Residuals vs Fitted (Interaction Model)",
  pch = 20,
  col = "blue"
)
abline(h = 0, col = "red", lwd = 2) # Add horizontal reference line

```

```{r}
#Checking for Multicollinearity
install.packages("car")
library(car)

vif(ZHVI_model_simple)

```

Based on the output of the vif stats, we observe that the VIF for all variables is below 5 and is acceptable. We can move forward with adjusting the model based on this. Let us try and test the model by splitting the data by our newly created variable, year. We will train our data on the years 2021 and 2022 and test it on the year 2023. 

```{r}
#Split into training and testing by year
training_data <- subset(housing_data, year %in% c(2021, 2022))
testing_data <- subset(housing_data, year == 2023)

ZHVI_model_train <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank, data = training_data)
predictions <- predict(ZHVI_model_train, newdata = testing_data)



valid_data <- na.omit(data.frame(actuals, predictions))


valid_actuals <- valid_data$actuals
valid_predictions <- valid_data$predictions

# Calculate residuals
valid_residuals <- valid_actuals - valid_predictions

# Compute performance metrics
MAE <- mean(abs(valid_residuals))   # Mean Absolute Error
RMSE <- sqrt(mean(valid_residuals^2)) # Root Mean Squared Error
R2 <- cor(valid_predictions, valid_actuals)^2  # R-squared

cat("MAE:", MAE, "\n")
cat("RMSE:", RMSE, "\n")
cat("R-squared:", R2, "\n")

#Scatter plot
plot(
  test_data_2023$log_of_ZHVI, test_data_2023$predicted,
  xlab = "Actual Log(ZHVI)", ylab = "Predicted Log(ZHVI)",
  main = "Actual vs Predicted Log(ZHVI) for 2023",
  pch = 16, col = "blue"
)
abline(0, 1, col = "red", lwd = 2) # Add a 45-degree line for perfect prediction

# Line plot using ggplot2
library(ggplot2)
ggplot(test_data_2023, aes(x = 1:nrow(test_data_2023))) +
  geom_line(aes(y = log_of_ZHVI, color = "Actual")) +
  geom_line(aes(y = predicted, color = "Predicted")) +
  labs(
    x = "Index",
    y = "Log(ZHVI)",
    title = "Trends in Actual vs Predicted Log(ZHVI) for 2023"
  ) +
  scale_color_manual(values = c("Actual" = "blue", "Predicted" = "red")) +
  theme_minimal()
```
To fully test how our model performs on unseen data, we decided to split the data into training and testing sets based on years as mentioned before. As shown above, we gave the years 2021-2022 into the training set and 2023 into the test set to observe how well our model will work on unseen data. The result is the model explains 56% of the variance in log(ZHVI) with a MAE of 0.293 and RMSE of 0.39. 

The results suggest that housing data in the real world can be very complex. Things like income, housing supply, houshold size, crime, and more were not included in the data Zillow provides from their website directly. This could explain the plateu we're witnessing when adding more variables from what we presently have.


**Testing the same on the model with interaction terms**
```{r}
# Split into training and testing by year
training_data <- subset(housing_data, year %in% c(2021, 2022))
testing_data <- subset(housing_data, year == 2023)

# Fit the model with interaction terms
ZHVI_model_interaction_train <- lm(log_of_ZHVI ~ year * State * Metro * SizeRank, data = training_data)

# Make predictions on the testing data
predictions_interaction <- predict(ZHVI_model_interaction_train, newdata = testing_data)

# Actual values for testing
actuals_interaction <- testing_data$log_of_ZHVI

# Calculate residuals
test_residuals_interaction <- actuals_interaction - predictions_interaction

# Evaluate performance using MAE, RMSE, and R-squared
MAE_interaction <- mean(abs(test_residuals_interaction))   # Mean Absolute Error
RMSE_interaction <- sqrt(mean(test_residuals_interaction^2)) # Root Mean Squared Error
R2_interaction <- cor(predictions_interaction, actuals_interaction)^2  # R-squared on test data

# Print results
cat("MAE (Interaction Model):", MAE_interaction, "\n")
cat("RMSE (Interaction Model):", RMSE_interaction, "\n")
cat("R-squared (Interaction Model):", R2_interaction, "\n")

# Prepare testing data with predictions for further analysis
testing_data$predicted <- predictions_interaction
```


To move forward with modeling this dataset, we need to determine the following: 

1. Is linear regression the problem for this dataset? (Is the model type too simple for our dataset)

2. Can we witness a higher r-squared value by using another model type?

3. How can we build a model that has stronger predictive power for our target variable? 

Let us test if the linear model itself is the problem by using random forest. Random forest is a bit more suited for complex models than the linear model, so naturally we could expect a better explanation of the variance in our target variable. 

```{r}
# Load necessary library for random forest
library(randomForest)

# Remove rows with missing values
training_data_clean <- na.omit(training_data)
testing_data_clean <- na.omit(testing_data)

# Check for missing values after removing them
sum(is.na(training_data_clean))
sum(is.na(testing_data_clean))

# Random forest with the same variables
rf_model_continuous <- randomForest(
  log_of_ZHVI ~ year + State + Metro + SizeRank,
  data = training_data_clean,
  importance = TRUE,  # To calculate variable importance
  ntree = 100         # Number of trees
)



# Predictions
rf_predictions_continuous <- predict(rf_model_continuous, newdata = testing_data_clean)

# Performance Metrics
rf_residuals_continuous <- testing_data_clean$log_of_ZHVI - rf_predictions_continuous
MAE_rf_continuous <- mean(abs(rf_residuals_continuous))
RMSE_rf_continuous <- sqrt(mean(rf_residuals_continuous^2))
R2_rf_continuous <- cor(rf_predictions_continuous, testing_data_clean$log_of_ZHVI)^2

# Print metrics
cat("Random Forest with Continuous SizeRank:\n")
cat("MAE:", MAE_rf_continuous, "\nRMSE:", RMSE_rf_continuous, "\nR-squared:", R2_rf_continuous, "\n")

# Variable Importance
importance(rf_model_continuous)
varImpPlot(rf_model_continuous)

```
We observe from the random forest model explains slightly more of the variance, but doesn't really have better metrics in MAE or RMSE. It's possible the data relationships aren't totally linear. We do observe that Metro and State are very important features in explaining log(ZHVI).

What we can conclude from this is that again, housing data in the real world is very complex.

```{r}
library(caret)

training_data_clean <- na.omit(training_data)

# Set up cross-validation
tuneGrid <- expand.grid(mtry = c(1, 2, 3, 4))

# Train the model using cross-validation
rf_model_tuned <- train(
  log_of_ZHVI ~ year + State + Metro + SizeRank,
  data = training_data_clean,
  method = "rf",        # Random Forest method
  trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation
  tuneGrid = tuneGrid,   # Use the tuning grid for mtry
  importance = TRUE
)

# Check the best model and performance
print(rf_model_tuned)
```




```{r}

library(caret)


# Set up cross-validation with 10 folds
train_control <- trainControl(method = "cv", number = 10)

# Fit the model with City included
model_with_city <- train(log_of_ZHVI ~ year + State + Metro + City, data = housing_data,
                         method = "lm", trControl = train_control)

# View the results of cross-validation
print(model_with_city)


```
Result: Error in na.fail.default(list(log_of_ZHVI = c(12.8550329145667, 12.6063557512654,  : 
  missing values in object


```{r}
library(leaps)

# Forward stepwise selection
forward_selection <- regsubsets(
  log_of_ZHVI ~ year + State + Metro + City + SizeRank, # Full set of predictors
  data = housing_data,        # Your dataset
  method = "forward",         # Specify forward stepwise selection
  nvmax = 10                  # Maximum number of predictors to include
)

# Summary of the forward selection process
summary_forward <- summary(forward_selection)

# Display the results
print(summary_forward)

# Get the best model for each number of predictors
best_models <- summary_forward$which
print(best_models)

# Plotting the adjusted R-squared to visualize the best number of predictors
plot(summary_forward$adjr2, type = "b", xlab = "Number of Predictors", ylab = "Adjusted R-squared")

```

```{r}
forward_model_with_city <- regsubsets(
    log_of_ZHVI ~ year + State + Metro + SizeRank + City,
    data = housing_data,
    nvmax = 12,  # Adjust if you want to limit the number of predictors
    method = "forward"
)
summary_forward_city <- summary(forward_model_with_city)

coef(forward_model_with_city, which.max(summary_forward_city$adjr2))
```

After testing  this, we can observe that overfitting is a problem with models of this dataset. Many of the variables given to us in this dataset are geographical. So for example, state, city, metro, and zipcode are all geographical measures of different degrees. Including all, or certain variables with a high number of levels introduces our model to noise and leads to overcomplexity and overfitting. 


```{r}
library(caret)
library(ggplot2)
library(lattice)

housing_data_clean <- na.omit(housing_data)


control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation



model_with_city <- train(log_of_ZHVI ~ year + State + Metro + City, 
                         data = housing_data_clean, 
                         method = "lm", 
                         trControl = control)

model_without_city <- train(log_of_ZHVI ~ year + State + Metro, 
                             data = housing_data_clean, 
                             method = "lm", 
                             trControl = control)


print(model_with_city)
print(model_without_city)
```
Code took forever to run further proving that many of the variables with an incredibly high level of factors have led to longer computation times, overfitting, and more model complexity.




```{r}
# Model with City
summary(lm(log_of_ZHVI ~ year + State + Metro + City, data = housing_data))

# Model without City
summary(lm(log_of_ZHVI ~ year + State + Metro, data = housing_data))

```

Result: The model with city had a multiple R-squared of 0.93 and an adjusted R-squared of 0.928. It also returned "5 not defined because of singularities". Given the many levels of some of the variables, it might be better to just simplify the model.

```{r}
# Load the car package
library(car)

# Fit the linear model
housing_model <- lm(log_of_ZHVI ~ year + State + Metro + City, data = housing_data)

# Calculate VIF for each predictor
vif_values <- vif(housing_model)

# View the VIF values
print(vif_values)
```

Testing for multicollinearity in this model with City included we get the error "there are aliased coefficients in the model". An error that occurs when there is perfrect multicollinearity in the model.

```{r}
housing_data <- na.omit(housing_data)


# Split the data into training and testing sets
set.seed(123)  # Set seed for reproducibility
trainIndex <- createDataPartition(housing_data$log_of_ZHVI, p = 0.8, list = FALSE)
train_data <- housing_data[trainIndex, ]
test_data <- housing_data[-trainIndex, ]

# Load necessary libraries
library(caret)
library(glmnet)

# Set up cross-validation control
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Ridge regression model (L2 regularization)
model_ridge <- train(log_of_ZHVI ~ year + State + Metro + City, 
                     data = train_data,  # Use the training set
                     method = "glmnet", 
                     trControl = control, 
                     tuneGrid = expand.grid(alpha = 0, lambda = seq(0.0001, 1, length = 100)))

# Print model results
print(model_ridge)
```


Based on the many different steps we've tried by now, we can see many of our variables we thought would contribute meaningfully to our linear model has led to problems in overfitting, complexity, and computation. So we decided maybe to take a different approach and add some variables Zillow didn't give to us before. 


```{r}
# Load ACS data and your housing dataset
acs_data <- read.csv("Merged_ZHVI_ACS.csv")
housing_data <- read.csv("C:/Users/shime/Downloads/zhvi-all-homes_md-va-dc_2021-2023.csv")

# Convert 'zhvi' in ACS data: Remove '$' and commas, then convert to numeric
acs_data$zhvi <- as.numeric(gsub("[\\$,]", "", acs_data$zhvi))

# Ensure 'RegionName' in housing_data matches 'zipcode' in acs_data
housing_data$RegionName <- as.character(housing_data$RegionName)
acs_data$zipcode <- as.character(acs_data$zipcode)

# Merge datasets using 'RegionName' and 'zipcode' as the key
merged_data <- merge(housing_data, acs_data, by.x = "RegionName", by.y = "zipcode", all.x = TRUE)

# Feature Engineering: Add derived variables
merged_data$income_per_household <- merged_data$averageincome / merged_data$households
#Check if average_income is skewed
hist(merged_data$averageincome, main = "Histogram of Average Income", xlab = "Average Income")
merged_data$log_averageincome <- log(merged_data$averageincome)

# Save the enhanced dataset
write.csv(merged_data, "enhanced_housing_data.csv", row.names = FALSE)

```

We have merged data from ACS(American Community Survey) under the U.S. Census Bureau to bring in two new variables in our dataset. The variables we will be focusing on are the average household income in a given zipcode and the number of homes in a given zip code. Each of the variables aren't monitored monthly like the ZHVI data we had. Another problem is that the variables only go up until 2022. The data for 2023 isn't readily available yet at the time of writing.

The goal in adding these variables is for more of the variance in log(ZHVI) to be explained. From the context of housing as a topic, we know supply is said to have an impact on price as well as average household income. 

Modeling should check if these variabels are statistically significant and if they have strong coefficneints/impacts on the target variable.


```{r}
#Building the new linear model with the two new variables from the merged data
new_housing_model <- lm(log_of_ZHVI ~ averageincome + households + State + Metro + SizeRank, data = merged_data)
summary(new_housing_model)

```

Compared to the original linear model with only geographical predictors, this model is significantly improved. The Multiple R-squared is at 0.763 and the adjusted is the same. 

**CHECK THIS BEFORE SUBMISSION**
Breaking down coefficients: A 1% increase in the average income corresponds to an increase in the log_ZHVI by about 0.86%. To no surprise coefficients for MD and VA are negative meaning they have a lower log_of_ZHVI compared to the reference state of DC.

```{r}
interaction_model <- lm(log_of_ZHVI ~ averageincome * households + averageincome * State + averageincome * Metro + SizeRank, data = merged_data)
summary(interaction_model)
```

The interaction term between log(averageincome) and households is significant, with a p-value of 1.5e-06. This suggests that the relationship between average income and ZHVI may depend on the number of households.

Many interaction terms between log(averageincome) and the State or Metro variables are also significant, indicating that the effect of income on ZHVI differs across various regions and metros.

These results support the idea that the relationship between income and housing values is not uniform across different areas, and adjusting for these interactions could improve model accuracy.

The multiple and adjusted r-squared are now 0.789.


**More testing for new model without interaction terms**
```{r}
# Split into training and testing by year
#training_data <- subset(merged_data, year %in% c(2021, 2022))
#testing_data <- subset(merged_data, year == 2023)

# Check for NAs in the testing data
sum(is.na(testing_data))

# Fit the linear model
new_housing_model <- lm(log_of_ZHVI ~ averageincome + households + State + Metro + SizeRank, data = training_data)

# Predictions on testing data
predictions_new <- predict(new_housing_model, newdata = testing_data)

# Actual values and residuals
actuals_new <- testing_data$log_of_ZHVI
test_residuals_new <- actuals_new - predictions_new

# Error metrics
MAE_new <- mean(abs(test_residuals_new))   # Mean Absolute Error
RMSE_new <- sqrt(mean(test_residuals_new^2)) # Root Mean Squared Error
R2_new <- cor(predictions_new, actuals_new)^2  # R-squared on test data

cat("MAE:", MAE_new, "\n")
cat("RMSE:", RMSE_new, "\n")
cat("R-squared:", R2_new, "\n")

# Scatter plot for Actual vs Predicted Log(ZHVI) for 2023
plot(
  testing_data$log_of_ZHVI, predictions_new,
  xlab = "Actual Log(ZHVI)", ylab = "Predicted Log(ZHVI)",
  main = "Actual vs Predicted Log(ZHVI) for 2023 (New Model)",
  pch = 16, col = "blue"
)
abline(0, 1, col = "red", lwd = 2) # Add a 45-degree line for perfect prediction
```



**Placeholder for regression trees and more**

```{r}








```




##IGNORE CODE BELOW FOR NOW##

#Let's try to see if SizeRank will add anything to the model 

```{r}
ZHVI_model_sizerank <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank, data = housing_data)
summary(ZHVI_model_sizerank)
```

From the result we observe, SizeRank offers almost nothing to the model. The R-squared and adjusted R-squared values have hardly changed at all. The coefficient for the value is also negligible. The p-value says the coefficient is statistically significant, but likely because of the large sample size. The RSE is the same too, so the model hasn't really improved. 

```{r}
housing_data$SizeRank2 <- as.factor(housing_data$SizeRank)
ZHVI_model_sizerank2 <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank2, data = housing_data)
summary(ZHVI_model_sizerank2)
```
After converting SizeRank into a factor variable and placing it into the model, the r-squared and mutliple r-squared shot up. Likely overfitting. 


```{r residuals without NAs being removed}
ZHVI_model <- lm(log_of_ZHVI ~ year + State + Metro, data = housing_data, na.action = na.exclude)
housing_data$residuals <- residuals(ZHVI_model)
housing_data$fitted_values <- fitted(ZHVI_model)

library(ggplot2)
ggplot(housing_data, aes(x = fitted_values, y = residuals)) +
  geom_point(alpha = 0.4) +                # Scatterplot of residuals
  geom_smooth(method = "loess", col = "red") +  # Lowess line
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Zero line
  labs(title = "Residuals vs Fitted Values (Handling NAs)",
       x = "Fitted Values (log of ZHVI)",
       y = "Residuals") +
  theme_minimal()


```


```{r residual plots with NA removed} 

model_data <- model.frame(ZHVI_model)

# Add residuals and fitted values to this dataset
model_data$residuals <- residuals(ZHVI_model)
model_data$fitted_values <- fitted(ZHVI_model)

# Create Residual vs. Fitted Plot
library(ggplot2)

ggplot(model_data, aes(x = fitted_values, y = residuals)) +
  geom_point(alpha = 0.4) +                # Scatterplot of residuals
  geom_smooth(method = "loess", col = "red") +  # Lowess line
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +  # Zero line
  labs(title = "Residuals vs Fitted Values",
       x = "Fitted Values (log of ZHVI)",
       y = "Residuals") +
  theme_minimal()

```
    
    
    
```{r}
ZHVI_model_interaction <- lm(log_of_ZHVI ~ year * State + Metro, data = housing_data)
summary(ZHVI_model_interaction)

```



```{r}
ZHVI_model_interaction2 <- lm(log_of_ZHVI ~ year * State * Metro, data = housing_data)
summary(ZHVI_model_interaction2)
```


```{r}
install.packages("car")  
library(car)

vif(ZHVI_model)
```


```{r}
#Too slow probably because there are too many levels to the factor
ZHVI_model_noMetro <- lm(log_of_ZHVI ~ year + State + City, data = housing_data)
print(ZHVI_model_noMetro)

```

**ANOVA testing models**
```{r}
anova(ZHVI_model, ZHVI_model_interaction, ZHVI_model_interaction2)

```
Based on the anova testing adding interaction terms improved the model from the initial model, ZHVI_model. So far model 3 with all 3 variables of year, state and metro and their interactions is the best model. However, can we improve it? 



**Testing more variables in the model**
```{r ridge regression}
# Install the package if you haven't already
install.packages("glmnet")

# Load the package
library(glmnet)

# Prepare the data
housing_data <- housing_data[!is.na(housing_data$ZHVI), ]

x <- model.matrix(log_of_ZHVI ~ year + State + Metro + SizeRank, data = housing_data)[, -1]  # Matrix form without the intercept
y <- housing_data$log_of_ZHVI

# Fit the Ridge model (alpha = 0 for Ridge)
ridge_model <- glmnet(x, y, alpha = 0)

# Check the coefficients
print(coef(ridge_model))

# Cross-validation for Ridge
cv_ridge <- cv.glmnet(x, y, alpha = 0)
plot(cv_ridge)

# Best lambda (regularization strength)
cv_ridge$lambda.min

#Use the ideal lambda to create a new ridge model
ridge_final_model <- glmnet(x,y, alpha = 0, labda = cv_ridge$lambda.min)

#coefficients for ridge
print(coef(ridge_final_model))

predictions <- predict(ridge_final_model, s = cv_ridge$lambda.min, newx = x)
mse <- mean((y - predictions)^2)
cat("Mean Squared Error for Ridge Model:", mse, "\n")

#Compare to linear model 
baseline_model <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank, data = housing_data)
baseline_predictions <- predict(baseline_model, newdata = housing_data)
baseline_mse <- mean((housing_data$log_of_ZHVI - baseline_predictions)^2)
cat("MSE for Baseline Linear Model:", baseline_mse, "\n")

```
Turns out the ridge  regression model isn't more effective than the original linear model. The MSE is lower with the original model.


```{r}

# Install the randomForest package
install.packages("randomForest")
library(randomForest)

# Fit a Random Forest model
rf_model <- randomForest(log_of_ZHVI ~ year + State + Metro + SizeRank, data = housing_data)

# Print model summary
print(rf_model)

# Plot the importance of variables
plot(rf_model)


```






Testing(ignore the code below this)
```{r}
training_data <- subset(housing_data, year %in% c(2021, 2022))
testing_data <- subset(housing_data, year == 2023)

# Model with continuous SizeRank
model_continuous <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank, data = training_data)

# Predictions
predictions_continuous <- predict(model_continuous, newdata = testing_data)

# Performance Metrics
actuals <- testing_data$log_of_ZHVI
residuals_continuous <- actuals - predictions_continuous

MAE_continuous <- mean(abs(residuals_continuous))
RMSE_continuous <- sqrt(mean(residuals_continuous^2))
R2_continuous <- cor(predictions_continuous, actuals)^2

# Print results
cat("Continuous SizeRank:\n")
cat("MAE:", MAE_continuous, "\nRMSE:", RMSE_continuous, "\nR-squared:", R2_continuous, "\n")



```


```{r}

# Recreate SizeRank_category for housing_data
percentiles <- quantile(housing_data$SizeRank, probs = c(0, 0.1, 0.3, 0.6, 1), na.rm = TRUE)

housing_data$SizeRank_category <- cut(
  housing_data$SizeRank,
  breaks = percentiles,
  labels = c("Top 10%", "10-30%", "30-60%", "60-100%"),
  include.lowest = TRUE
)

# Split housing_data into training and testing data again
training_data <- subset(housing_data, year %in% c(2021, 2022))
testing_data <- subset(housing_data, year == 2023)

# Check distribution of SizeRank_category in training and testing data
cat("Training Data Distribution:\n")
print(table(training_data$SizeRank_category))
cat("\nTesting Data Distribution:\n")
print(table(testing_data$SizeRank_category))

# Build the categorical model
model_categorical <- lm(log_of_ZHVI ~ year + State + Metro + SizeRank_category, data = training_data)

# Generate predictions for the testing data
predictions_categorical <- predict(model_categorical, newdata = testing_data)

# Calculate performance metrics
actuals <- testing_data$log_of_ZHVI
residuals_categorical <- actuals - predictions_categorical

MAE_categorical <- mean(abs(residuals_categorical))
RMSE_categorical <- sqrt(mean(residuals_categorical^2))
R2_categorical <- cor(predictions_categorical, actuals)^2

# Print results
cat("Categorical SizeRank:\n")
cat("MAE:", MAE_categorical, "\nRMSE:", RMSE_categorical, "\nR-squared:", R2_categorical, "\n")

```

```{r}
# Regression Tree
install.packages("rpart")
install.packages("rpart.plot")
install.packages("caret")
```

```{r}
library(rpart)
library(rpart.plot)
library(caret)

set.seed(123)
selected_columns <- c("SizeRank", "ZHVI")

train_index <- createDataPartition(dc_data$ZHVI, p = 0.7, list = FALSE)

train_data <- dc_data[train_index, selected_columns]
test_data <- dc_data[-train_index, selected_columns]

tree_model <- rpart(ZHVI ~ SizeRank, data = train_data, method = "anova")

rpart.plot(tree_model, type = 2, extra = 1, main = "Regression Tree for ZHVI ~ SizeRank (DC)")

set.seed(123)
md_data_clean <- na.omit(md_data[, c("SizeRank", "ZHVI")]) 
train_index1 <- createDataPartition(md_data_clean$ZHVI, p = 0.7, list = FALSE)

train_data1 <- md_data_clean[train_index1, selected_columns]
test_data1 <- md_data_clean[-train_index1, selected_columns]

tree_model1 <- rpart(ZHVI ~ SizeRank, data = train_data1, method = "anova")

rpart.plot(tree_model1, type = 2, extra = 1, main = "Regression Tree for ZHVI ~ SizeRank (MD)")

set.seed(123)
va_data_clean <- na.omit(va_data[, c("SizeRank", "ZHVI")]) 
train_index2 <- createDataPartition(va_data_clean$ZHVI, p = 0.7, list = FALSE)

train_data2 <- va_data_clean[train_index, selected_columns]
test_data2 <- va_data_clean[-train_index, selected_columns]

tree_model2 <- rpart(ZHVI ~ SizeRank, data = train_data2, method = "anova")

rpart.plot(tree_model2, type = 2, extra = 1, main = "Regression Tree for ZHVI ~ SizeRank (VA)")
```

